{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7178fa69410bb98e9297ac511c7f4ae7",
     "grade": false,
     "grade_id": "cell-0a8316b039d048ad",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Number of points for this notebook:</b> 0.5\n",
    "<br>\n",
    "<b>Deadline:</b> March 2, 2020 (Monday). 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 1.1. Logistic regression\n",
    "\n",
    "The goal of this exercise is to get familiar with the basics of PyTorch and train a simple logistic regression model.\n",
    "\n",
    "If you are not familiar with PyTorch, there is a number of good tutorials [here](https://pytorch.org/tutorials/index.html). We recommend the following ones:\n",
    "* [What is PyTorch?](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)\n",
    "* [Autograd: Automatic Differentiation](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)\n",
    "* [Learning PyTorch with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n",
    "* [Neural Networks](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e2970339980ef7d85c3754662c4ee8",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tools\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# When running on your own computer, you can specify the data directory by:\n",
    "# data_dir = tools.select_data_dir('/your/local/data/directory')\n",
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device which you are going to use for training\n",
    "#device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc5c0195206dae40876fe429916217c4",
     "grade": false,
     "grade_id": "cell-70232a39ccf9c751",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce96096aea16b4ee895f3d55210a51e3",
     "grade": false,
     "grade_id": "cell-ce13efdf413792bd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "We are going to use *winequality* dataset which contains red and white vinho verde wine samples rated by experts from 0 to 10 (obtained from [here](https://archive.ics.uci.edu/ml/datasets/wine+quality))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da2cf1c55486aec8ef40e6b3b609a553",
     "grade": false,
     "grade_id": "cell-4bba5619c4f19119",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5197, 11]) torch.Size([5197])\n"
     ]
    }
   ],
   "source": [
    "trainset = data.WineQuality(data_dir, train=True, normalize=False)\n",
    "train_inputs, train_targets = trainset.tensors\n",
    "print(train_inputs.shape, train_targets.shape)\n",
    "\n",
    "testset = data.WineQuality(data_dir, train=False, normalize=False)\n",
    "test_inputs, test_targets = testset.tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed267bfabcf861e6d178cdade0ab4fc8",
     "grade": false,
     "grade_id": "cell-a560530c488feda3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will transform the task into a binary classification problem and try to predict if the quality of wine is greater or lower than 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8428ad236070895a92275ac6dda649f",
     "grade": false,
     "grade_id": "cell-1125b3591cf91eb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert to a binary classification problem\n",
    "train_targets = (train_targets >= 7).float().view(-1, 1)  \n",
    "test_targets = (test_targets >= 7).float().view(-1, 1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "684bcb9986d4097fe8cfda071512782d",
     "grade": false,
     "grade_id": "cell-bfce5fedb6d8d6ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The optimization problem is often easier when the model inputs are normalized to zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f8769823a27c3b2b7b1587c8678ddd1",
     "grade": false,
     "grade_id": "cell-2b752e13ecf16ed6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Normalize inputs to zero mean and unit variance\n",
    "mean = train_inputs.mean(dim=0)\n",
    "std = train_inputs.std(dim=0)\n",
    "scaler = lambda x: (x - mean.to(x.device)) / std.to(x.device)\n",
    "\n",
    "train_inputs = scaler(train_inputs)\n",
    "test_inputs = scaler(test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef3872ff2e4806afd56e9ffee54c3062",
     "grade": false,
     "grade_id": "cell-fff001b57c687c28",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Logistic regression classifier\n",
    "\n",
    "Logistic regression is a linear model which is used to solve a binary classification task. According to the model, the probability that example $x$ belongs to class 1 is computed as\n",
    "$$\n",
    "  p(y=1 \\mid x) = \\sigma (w^T x + b)\n",
    "$$\n",
    "where vector $w$ and scalar $b$ are the parameters of the model and $\\sigma(\\cdot)$ is the sigmoid function.\n",
    "\n",
    "In the cell below, your task is to specify the logistic regression model as a PyTorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31e7d16342ad0b0deaa7cda3aca6b501",
     "grade": false,
     "grade_id": "LogisticRegression",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_inputs=11):\n",
    "        # YOUR CODE HERE\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.w = nn.Linear(n_inputs, 1, bias=True)\n",
    "#         raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (n_samples, n_inputs): Model inputs.\n",
    "        \n",
    "        Returns:\n",
    "          y of shape (n_samples, 1): Model outputs.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        out = torch.sigmoid(self.w(x))\n",
    "        return out\n",
    "#         raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41b9e6438b4f91533cd59c5e9b44536c",
     "grade": false,
     "grade_id": "cell-753b010f28d3b080",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(\n",
      "  (w): Linear(in_features=11, out_features=1, bias=True)\n",
      ")\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Let us create the network and make sure it can process a random input of the right shape\n",
    "def test_logreg_shapes():\n",
    "    n_inputs = 11\n",
    "    n_samples = 10\n",
    "    model = LogisticRegression()\n",
    "    print(model)\n",
    "    y = model(torch.randn(n_samples, n_inputs))\n",
    "    assert y.shape == torch.Size([n_samples, 1]), f\"Bad y.shape: {y.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_logreg_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f4dbd3477abdf7b3f351ecfc68f7e48",
     "grade": false,
     "grade_id": "cell-70cbd420870116d7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Train the model\n",
    "\n",
    "Next we will train the logistic regression model. The model is trained by minimizing the following loss:\n",
    "$$\n",
    "  \\text{loss} = \\sum_i - [ t_i \\log(y_i) + (1−t_i) \\log(1−y_i)]\n",
    "$$\n",
    "where $t_i$ is the target class label and $y_i$ is the output of the logistic regression classifier for training sample $x_i$.\n",
    "This loss function is implemented in function [`binary_cross_entropy`](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.binary_cross_entropy) of PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "212d270ac0c2335befac420085ec8e9e",
     "grade": false,
     "grade_id": "cell-8fc8338e174a33bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training loop\n",
    "\n",
    "Your task is to implement the training loop.\n",
    "You may find it useful to look at [this tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py).\n",
    "Your should have the following steps:\n",
    "* Set all gradient values to zeros.\n",
    "* Calculate the output of the model for all training examples.\n",
    "* Calculate the binary cross entropy loss (see [`binary_cross_entropy`](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.binary_cross_entropy)).\n",
    "* Backpropagate the gradients: compute the gradients of the loss wrt to all the parameters of the model.\n",
    "* Update the parameters of the model using the chosen optimizer.\n",
    "\n",
    "Recommended hyperparameters:\n",
    "* [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate 0.001.\n",
    "* You can process the data in the full-batch model (computing the gradients using all training data).\n",
    "* Number of iterations (parameter updates): 8000.\n",
    "\n",
    "Hints:\n",
    "- We recommend you to print the classification accuracy during training. You can compute the accuracy using function `compute_accuracy`.\n",
    "- The accuracy on the training set should be above 0.8. The accuracy on the test set should be above 0.79."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "068c30c37139ddf0aa53f241da9c87d4",
     "grade": false,
     "grade_id": "cell-cdcbcdc46a4f7b23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the accuracy of the model on the given dataset\n",
    "def compute_accuracy(model, inputs, targets):\n",
    "    with torch.no_grad():\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = (model.forward(inputs) > 0.5).float()\n",
    "        accuracy = (outputs == targets).sum().float() / targets.numel()\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "057fd147772a5af9c4dfb64cd5488df7",
     "grade": false,
     "grade_id": "cell-5c8792dd4f0928e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (w): Linear(in_features=11, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model\n",
    "model = LogisticRegression()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f6c8b0a2ed0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.w.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d69631670e1f170cfac95fe177dc3a9",
     "grade": false,
     "grade_id": "cell-692ef1b990bd1bbc",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.4283190071582794, acc: 0.8183567523956299\n",
      "\n",
      "epoch: 101, loss: 0.41835731267929077, acc: 0.8172022104263306\n",
      "\n",
      "epoch: 201, loss: 0.4109708368778229, acc: 0.8175870776176453\n",
      "\n",
      "epoch: 301, loss: 0.4052406847476959, acc: 0.8214354515075684\n",
      "\n",
      "epoch: 401, loss: 0.40080833435058594, acc: 0.8241292834281921\n",
      "\n",
      "epoch: 501, loss: 0.39739757776260376, acc: 0.8258610963821411\n",
      "\n",
      "epoch: 601, loss: 0.3947867453098297, acc: 0.8248989582061768\n",
      "\n",
      "epoch: 701, loss: 0.3928021490573883, acc: 0.8252838253974915\n",
      "\n",
      "epoch: 801, loss: 0.3913048207759857, acc: 0.8248989582061768\n",
      "\n",
      "epoch: 901, loss: 0.39018577337265015, acc: 0.8248989582061768\n",
      "\n",
      "epoch: 1001, loss: 0.389356404542923, acc: 0.8250914216041565\n",
      "\n",
      "epoch: 1101, loss: 0.3887494206428528, acc: 0.8248989582061768\n",
      "\n",
      "epoch: 1201, loss: 0.3883114755153656, acc: 0.8250914216041565\n",
      "\n",
      "epoch: 1301, loss: 0.3879990577697754, acc: 0.8250914216041565\n",
      "\n",
      "epoch: 1401, loss: 0.38778045773506165, acc: 0.8247065544128418\n",
      "\n",
      "epoch: 1501, loss: 0.38763004541397095, acc: 0.8248989582061768\n",
      "\n",
      "epoch: 1601, loss: 0.38752880692481995, acc: 0.8243217468261719\n",
      "\n",
      "epoch: 1701, loss: 0.38746127486228943, acc: 0.8245141506195068\n",
      "\n",
      "epoch: 1801, loss: 0.38741785287857056, acc: 0.8247065544128418\n",
      "\n",
      "epoch: 1901, loss: 0.38739094138145447, acc: 0.8248989582061768\n",
      "\n",
      "epoch: 2001, loss: 0.3873743414878845, acc: 0.8252838253974915\n",
      "\n",
      "epoch: 2101, loss: 0.3873642385005951, acc: 0.8252838253974915\n",
      "\n",
      "epoch: 2201, loss: 0.38735857605934143, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 2301, loss: 0.3873552083969116, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 2401, loss: 0.38735365867614746, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 2501, loss: 0.38735267519950867, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 2601, loss: 0.38735154271125793, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 2701, loss: 0.38735231757164, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 2801, loss: 0.38735222816467285, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 2901, loss: 0.38735124468803406, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 3001, loss: 0.387351930141449, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 3101, loss: 0.38735231757164, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 3201, loss: 0.3873518109321594, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 3301, loss: 0.3873520493507385, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 3401, loss: 0.3873516023159027, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 3501, loss: 0.38735178112983704, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 3601, loss: 0.38735201954841614, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 3701, loss: 0.38735219836235046, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 3801, loss: 0.3873520791530609, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 3901, loss: 0.3873516619205475, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 4001, loss: 0.3873520791530609, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 4101, loss: 0.38735175132751465, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 4201, loss: 0.387351930141449, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 4301, loss: 0.3873519003391266, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 4401, loss: 0.3873521387577057, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 4501, loss: 0.38735231757164, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 4601, loss: 0.38735198974609375, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 4701, loss: 0.3873521685600281, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 4801, loss: 0.38735172152519226, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 4901, loss: 0.3873521387577057, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 5001, loss: 0.38735151290893555, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 5101, loss: 0.3873520493507385, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 5201, loss: 0.38735231757164, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 5301, loss: 0.3873520791530609, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 5401, loss: 0.3873518407344818, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 5501, loss: 0.3873521089553833, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 5601, loss: 0.3873521089553833, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 5701, loss: 0.38735198974609375, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 5801, loss: 0.3873521387577057, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 5901, loss: 0.387351930141449, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 6001, loss: 0.3873520791530609, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 6101, loss: 0.38735198974609375, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 6201, loss: 0.3873520791530609, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 6301, loss: 0.38735195994377136, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 6401, loss: 0.38735198974609375, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 6501, loss: 0.3873519003391266, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 6601, loss: 0.3873521685600281, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 6701, loss: 0.3873520493507385, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 6801, loss: 0.3873521387577057, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 6901, loss: 0.3873519003391266, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 7001, loss: 0.3873521089553833, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 7101, loss: 0.3873521089553833, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 7201, loss: 0.3873521089553833, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 7301, loss: 0.3873521685600281, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 7401, loss: 0.3873524069786072, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 7501, loss: 0.3873521089553833, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 7601, loss: 0.3873519003391266, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 7701, loss: 0.3873521089553833, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 7801, loss: 0.38735198974609375, acc: 0.8254762291908264\n",
      "\n",
      "epoch: 7901, loss: 0.3873519003391266, acc: 0.8254762291908264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implement the training loop in this cell\n",
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    criterion = nn.functional.binary_cross_entropy\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(8000):\n",
    "        optim.zero_grad()\n",
    "        out = model.forward(train_inputs)\n",
    "        loss = criterion(out, train_targets)\n",
    "        if epoch%100==0:\n",
    "            print(\"epoch: {}, loss: {}, acc: {}\".format(epoch+1, loss, compute_accuracy(model,train_inputs,train_targets)))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "#     raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2aac76321cb810b2dd74605d36d8b004",
     "grade": false,
     "grade_id": "mlp_accuracy",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to save the model (type yes to confirm)? yes\n",
      "Model saved to 1_logreg.pth.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "if not skip_training:\n",
    "    tools.save_model(model, '1_logreg.pth')\n",
    "else:\n",
    "    model = LogisticRegression()\n",
    "    tools.load_model(model, '1_logreg.pth', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4d4655c77df492ab32285a787a5a55f",
     "grade": true,
     "grade_id": "test_accuracy",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.8061538338661194\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_accuracy(model, test_inputs, test_targets)\n",
    "print('Accuracy on test set:', accuracy.item())\n",
    "assert accuracy >= 0.79, 'Logistic regression classifier has poor accuracy.'\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03b89f820c6ce77c8d5119b10d39c912",
     "grade": true,
     "grade_id": "cell-7f56154b186d5c0e",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests LogisticRegression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
